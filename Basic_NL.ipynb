{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理\n",
    "* 普段が私たちが使用している言葉を自然言語と呼ぶ。\n",
    "\n",
    "#### シソーラス（類義語）\n",
    "* WordNet\n",
    "##### ・問題点\n",
    "* 時代の変化に対応するのが困難\n",
    "* 人の作業コストが高い\n",
    "* 単語の細かなニュアンスを表現できない\n",
    "\n",
    "##### カウントベースの手法\n",
    "* コーパス：大量のテキストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say Hello and how are you and I say Hello and I'm fine.\"\n",
    "text = text.replace(\".\", \" .\")#空白を作ることで一つの文字として認識される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say hello and how are you and i say hello and i'm fine .\n",
      "YOU SAY HELLO AND HOW ARE YOU AND I SAY HELLO AND I'M FINE .\n",
      "['You', 'say', 'Hello', 'and', 'how', 'are', 'you', 'and', 'I', 'say', 'Hello', 'and', \"I'm\", 'fine', '.']\n"
     ]
    }
   ],
   "source": [
    "lower_text = text.lower()#小文字\n",
    "print(lower_text)\n",
    "upper_text = text.upper()#大文字\n",
    "print(upper_text)\n",
    "split_text = text.split(' ')\n",
    "print(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'You', 1: 'say', 2: 'Hello', 3: 'and', 4: 'how', 5: 'are', 6: 'you', 7: 'I', 8: \"I'm\", 9: 'fine', 10: '.'}\n",
      "{'You': 0, 'say': 8, 'Hello': 8, 'and': 8, 'how': 4, 'are': 5, 'you': 6, 'I': 7, \"I'm\": 8, 'fine': 9, '.': 10}\n",
      "are\n"
     ]
    }
   ],
   "source": [
    "#カウントベースの処理\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for split in split_text:\n",
    "    new_id = len(word_to_id)#現在の長さを代入\n",
    "    word_to_id[split] = new_id \n",
    "    id_to_word[new_id] = split\n",
    "    \n",
    "print(id_to_word)\n",
    "print(word_to_id)\n",
    "print(id_to_word[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  8,  8,  8,  4,  5,  6,  8,  7,  8,  8,  8,  8,  9, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#コーパスとはテキストデータのこと\n",
    "corpus = [word_to_id[w] for w in split_text]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" .\")\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    word_to_id, id_to_word = {}, {}\n",
    "    for word in words:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 単語分散表現\n",
    "* RGBのようなベクトル表現\n",
    "\n",
    "#### 共起行列\n",
    "* 分布仮説に基づいて、単語をベクトルで表現する。（カウント）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/work/NaturalProcessing/deep-learning-from-scratch-2-master/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6 5 7 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.', 7: 'world'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6, 'world': 7}\n"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess\n",
    "text = \"You say goodbye and I say hello. Hello World.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)\n",
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#共起行列：分布仮説に基づいて単語をベクトルで表現する。\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#コサイン類似度（二つのベクトルがどれだけ同じ方向を向いているかを求める）\n",
    "#完全に同じ方向を向いている場合は１逆は−１\n",
    "def cos_similarity(x, y, eps=1e+8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess, create_co_matrix, cos_similarity\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id[\"you\"]]\n",
    "c1 = C[word_to_id[\"i\"]]\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
