{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vecの高速化\n",
    "* Embeddingレイヤの導入\n",
    "* Negative Samplingという新しい損失関数の導入。\n",
    "\n",
    "##### 問題点\n",
    "* 入力層がone-hot表現のため語彙数が増えるとそれ相応のメモリサイズが必要になる。→Embedding層の追加\n",
    "* 中間層の問題（多くの計算が必要になる）→Negative sampling \n",
    "\n",
    "#### Embeddingレイヤ\n",
    "* 単語IDに該当する行を抜き出すレイヤ(**Embedding層**)\n",
    "* 単語の密なベクトル表現を単語の埋め込み（Embedding）や単語の分散表現と呼ばれる。\n",
    "\n",
    "#### 中間層の問題\n",
    "* 多値分類から二値分類に変換する。（これはAであるか否かみたいな感じ）\n",
    "\n",
    "##### シグモイド関数と交差エントロピー誤差\n",
    "* 多値分類の場合は出力層にはソフトマックス関数、損失関数には交差エントロピー誤差\n",
    "* 二値分類の出力層にはシグモイド関数、損失関数には交差エントロピー誤差\n",
    "\n",
    "$$L=-(t\\log{y}+(1-t)\\log{(1-y)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]#特定の行を抜き出す\n",
    "        return out \n",
    "    \n",
    "#     def backward(self, dout):\n",
    "#         dW, = self.grads\n",
    "#         dW[...] = 0\n",
    "#         dW[self.idx] = dout\n",
    "#         return None\n",
    "\n",
    "#重複問題が発生するため、代入ではなく加算を行う。\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)#Numpyの方が早い\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "say\n",
      "['say' 'say' 'say' 'you' 'you']\n",
      "['.' 'you' 'say' 'hello' 'goodbye']\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(10))\n",
    "words = [\"you\", \"say\", \"goodbye\", \"I\", \"hello\", \".\"]\n",
    "print(np.random.choice(words))\n",
    "print(np.random.choice(words, size=5))\n",
    "print(np.random.choice(words, size=5, replace=False))\n",
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "print(np.random.choice(words, p=p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
